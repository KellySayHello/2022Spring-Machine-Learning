{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Homework 10 - Adversarial Attack**\n\nSlides: https://reurl.cc/7DDxnD\n\nContact: ntu-ml-2022spring-ta@googlegroups.com\n","metadata":{"id":"Q-n2e0BkhEKS","papermill":{"duration":0.020487,"end_time":"2022-05-17T13:20:35.827735","exception":false,"start_time":"2022-05-17T13:20:35.807248","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"## Enviroment & Download\n\nWe make use of [pytorchcv](https://pypi.org/project/pytorchcv/) to obtain CIFAR-10 pretrained model, so we need to set up the enviroment first. We also need to download the data (200 images) which we want to attack.","metadata":{"id":"9RX7iRXrhMA_","papermill":{"duration":0.018253,"end_time":"2022-05-17T13:20:35.864775","exception":false,"start_time":"2022-05-17T13:20:35.846522","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# set up environment\n!pip install pytorchcv\n!pip install imgaug\n\n# download\n!wget https://github.com/DanielLin94144/ML-attack-dataset/files/8167812/data.zip\n\n# unzip\n!unzip ./data.zip\n!rm ./data.zip","metadata":{"id":"d4Lw7urignqP","outputId":"257978c5-537b-44e1-c39d-0bded7ef9df2","papermill":{"duration":24.549942,"end_time":"2022-05-17T13:21:00.434222","exception":false,"start_time":"2022-05-17T13:20:35.88428","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-05-19T03:32:53.196686Z","iopub.execute_input":"2022-05-19T03:32:53.1972Z","iopub.status.idle":"2022-05-19T03:33:50.763006Z","shell.execute_reply.started":"2022-05-19T03:32:53.197151Z","shell.execute_reply":"2022-05-19T03:33:50.762077Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nbatch_size = 8","metadata":{"id":"5inbFx_alYjw","papermill":{"duration":1.769028,"end_time":"2022-05-17T13:21:02.230692","exception":false,"start_time":"2022-05-17T13:21:00.461664","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-05-19T03:34:01.976314Z","iopub.execute_input":"2022-05-19T03:34:01.976604Z","iopub.status.idle":"2022-05-19T03:34:01.981104Z","shell.execute_reply.started":"2022-05-19T03:34:01.976575Z","shell.execute_reply":"2022-05-19T03:34:01.980202Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Global Settings \n#### **[NOTE]**: Don't change the settings here, or your generated image might not meet the constraint.\n* $\\epsilon$ is fixed to be 8. But on **Data section**, we will first apply transforms on raw pixel value (0-255 scale) **by ToTensor (to 0-1 scale)** and then **Normalize (subtract mean divide std)**. $\\epsilon$ should be set to $\\frac{8}{255 * std}$ during attack.\n\n* Explaination (optional)\n    * Denote the first pixel of original image as $p$, and the first pixel of adversarial image as $a$.\n    * The $\\epsilon$ constraints tell us $\\left| p-a \\right| <= 8$.\n    * ToTensor() can be seen as a function where $T(x) = x/255$.\n    * Normalize() can be seen as a function where $N(x) = (x-mean)/std$ where $mean$ and $std$ are constants.\n    * After applying ToTensor() and Normalize() on $p$ and $a$, the constraint becomes $\\left| N(T(p))-N(T(a)) \\right| = \\left| \\frac{\\frac{p}{255}-mean}{std}-\\frac{\\frac{a}{255}-mean}{std} \\right| = \\frac{1}{255 * std} \\left| p-a \\right| <= \\frac{8}{255 * std}.$\n    * So, we should set $\\epsilon$ to be $\\frac{8}{255 * std}$ after ToTensor() and Normalize().","metadata":{"id":"hkQQf0l1hbBs","papermill":{"duration":0.027143,"end_time":"2022-05-17T13:21:02.285656","exception":false,"start_time":"2022-05-17T13:21:02.258513","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# the mean and std are the calculated statistics from cifar_10 dataset\ncifar_10_mean = (0.491, 0.482, 0.447) # mean for the three channels of cifar_10 images\ncifar_10_std = (0.202, 0.199, 0.201) # std for the three channels of cifar_10 images\n\n# convert mean and std to 3-dimensional tensors for future operations\nmean = torch.tensor(cifar_10_mean).to(device).view(3, 1, 1)\nstd = torch.tensor(cifar_10_std).to(device).view(3, 1, 1)\n\nepsilon = 8/255/std","metadata":{"id":"ACghc_tsg2vE","papermill":{"duration":2.767687,"end_time":"2022-05-17T13:21:05.080193","exception":false,"start_time":"2022-05-17T13:21:02.312506","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-05-19T03:33:50.774312Z","iopub.execute_input":"2022-05-19T03:33:50.774577Z","iopub.status.idle":"2022-05-19T03:33:50.785807Z","shell.execute_reply.started":"2022-05-19T03:33:50.774543Z","shell.execute_reply":"2022-05-19T03:33:50.785118Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"root = './data' # directory for storing benign images\n# benign images: images which do not contain adversarial perturbations\n# adversarial images: images which include adversarial perturbations","metadata":{"id":"uO8f0NmtlM63","papermill":{"duration":0.03468,"end_time":"2022-05-17T13:21:05.142362","exception":false,"start_time":"2022-05-17T13:21:05.107682","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-05-19T03:34:23.559719Z","iopub.execute_input":"2022-05-19T03:34:23.559989Z","iopub.status.idle":"2022-05-19T03:34:23.563593Z","shell.execute_reply.started":"2022-05-19T03:34:23.55996Z","shell.execute_reply":"2022-05-19T03:34:23.562859Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data\n\nConstruct dataset and dataloader from root directory. Note that we store the filename of each image for future usage.","metadata":{"id":"lhBJBAlKherZ","papermill":{"duration":0.03281,"end_time":"2022-05-17T13:21:05.203198","exception":false,"start_time":"2022-05-17T13:21:05.170388","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import os\nimport glob\nimport shutil\nimport numpy as np\nfrom PIL import Image\nfrom torchvision.transforms import transforms\nfrom torch.utils.data import Dataset, DataLoader\n\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize(cifar_10_mean, cifar_10_std)\n])\n\nclass AdvDataset(Dataset):\n    def __init__(self, data_dir, transform):\n        self.images = []\n        self.labels = []\n        self.names = []\n        '''\n        data_dir\n        ├── class_dir\n        │   ├── class1.png\n        │   ├── ...\n        │   ├── class20.png\n        '''\n        for i, class_dir in enumerate(sorted(glob.glob(f'{data_dir}/*'))):\n            images = sorted(glob.glob(f'{class_dir}/*'))\n            self.images += images\n            self.labels += ([i] * len(images))\n            self.names += [os.path.relpath(imgs, data_dir) for imgs in images]\n        self.transform = transform\n    def __getitem__(self, idx):\n        image = self.transform(Image.open(self.images[idx]))\n        label = self.labels[idx]\n        return image, label\n    def __getname__(self):\n        return self.names\n    def __len__(self):\n        return len(self.images)\n\nadv_set = AdvDataset(root, transform=transform)\nadv_names = adv_set.__getname__()\nadv_loader = DataLoader(adv_set, batch_size=batch_size, shuffle=False)\n","metadata":{"id":"VXpRAHz0hkDt","papermill":{"duration":0.264633,"end_time":"2022-05-17T13:21:05.498226","exception":false,"start_time":"2022-05-17T13:21:05.233593","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-05-19T03:34:25.478994Z","iopub.execute_input":"2022-05-19T03:34:25.479521Z","iopub.status.idle":"2022-05-19T03:34:25.500046Z","shell.execute_reply.started":"2022-05-19T03:34:25.479483Z","shell.execute_reply":"2022-05-19T03:34:25.499272Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Utils -- Benign Images Evaluation","metadata":{"id":"LnszlTsYrTQZ","papermill":{"duration":0.028147,"end_time":"2022-05-17T13:21:05.554713","exception":false,"start_time":"2022-05-17T13:21:05.526566","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# to evaluate the performance of model on benign images\ndef epoch_benign(model, loader, loss_fn):\n    model.eval()\n    train_acc, train_loss = 0.0, 0.0\n    for x, y in loader:\n        x, y = x.to(device), y.to(device)\n        yp = model(x)\n        loss = loss_fn(yp, y)\n        train_acc += (yp.argmax(dim=1) == y).sum().item()\n        train_loss += loss.item() * x.shape[0]\n    return train_acc / len(loader.dataset), train_loss / len(loader.dataset)","metadata":{"id":"5c_zZLzkrceE","papermill":{"duration":0.036253,"end_time":"2022-05-17T13:21:05.619692","exception":false,"start_time":"2022-05-17T13:21:05.583439","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-05-19T03:34:28.078937Z","iopub.execute_input":"2022-05-19T03:34:28.079767Z","iopub.status.idle":"2022-05-19T03:34:28.087456Z","shell.execute_reply.started":"2022-05-19T03:34:28.079726Z","shell.execute_reply":"2022-05-19T03:34:28.085837Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Utils -- Attack Algorithm","metadata":{"id":"_YJxK7YehqQy","papermill":{"duration":0.027788,"end_time":"2022-05-17T13:21:05.675437","exception":false,"start_time":"2022-05-17T13:21:05.647649","status":"completed"},"tags":[]}},{"cell_type":"code","source":"from pytorchcv.models.xdensenet_cifar import xdensenet40_2_k24_bc_svhn\n# perform fgsm attack\ndef fgsm(model, x, y, loss_fn, epsilon=epsilon):\n    x_adv = x.detach().clone() # initialize x_adv as original benign image x\n    x_adv.requires_grad = True # need to obtain gradient of x_adv, thus set required grad\n    loss = loss_fn(model(x_adv), y) # calculate loss\n    loss.backward() # calculate gradient\n    # fgsm: use gradient ascent on x_adv to maximize loss\n    grad = x_adv.grad.detach()\n    x_adv = x_adv + epsilon * grad.sign()\n    return x_adv\n\n# alpha and num_iter can be decided by yourself\nalpha = 0.8/255/std\ndef ifgsm(model, x, y, loss_fn, epsilon=epsilon, alpha=alpha, num_iter=20):\n    x_adv = x\n    # write a loop of num_iter to represent the iterative times\n    for i in range(num_iter):\n        # x_adv = fgsm(model, x_adv, y, loss_fn, alpha) # call fgsm with (epsilon = alpha) to obtain new x_adv\n        x_adv = x_adv.detach().clone()\n        x_adv.requires_grad = True # need to obtain gradient of x_adv, thus set required grad\n        loss = loss_fn(model(x_adv), y) # calculate loss\n        loss.backward() # calculate gradient\n        # fgsm: use gradient ascent on x_adv to maximize loss\n        grad = x_adv.grad.detach()\n        x_adv = x_adv + alpha * grad.sign()\n\n        x_adv = torch.max(torch.min(x_adv, x+epsilon), x-epsilon) # clip new x_adv back to [x-epsilon, x+epsilon]\n    return x_adv\n\ndef mifgsm(model, x, y, loss_fn, epsilon=epsilon, alpha=alpha, num_iter=20, decay=1.0):\n    x_adv = x\n    # initialze momentum tensor\n    momentum = torch.zeros_like(x).detach().to(device)\n    # write a loop of num_iter to represent the iterative times\n    grad = 0\n    for i in range(num_iter):\n        x_adv = x_adv.detach().clone()\n        x_adv.requires_grad = True # need to obtain gradient of x_adv, thus set required grad\n        loss = loss_fn(model(x_adv), y) # calculate loss\n\n        loss.backward(retain_graph=True) # calculate gradient\n        # TODO: Momentum calculation\n        grad = torch.autograd.grad(loss, x_adv ,retain_graph=False, create_graph=False)[0]\n        grad_norm = torch.norm(nn.Flatten()(grad), p =1, dim =1)\n        new_grad = grad / grad_norm.view([-1]+ [1]*(len(grad.shape)-1))\n        new_grad = momentum * decay+new_grad \n        momentum = new_grad\n\n        x_adv = x_adv.detach() + alpha*grad.sign()\n        x_adv = torch.max(torch.min(x_adv, x+epsilon), x-epsilon) # clip new x_adv back to [x-epsilon, x+epsilon]\n    return x_adv","metadata":{"id":"F_1wKfKyhrQW","papermill":{"duration":0.077266,"end_time":"2022-05-17T13:21:05.780011","exception":false,"start_time":"2022-05-17T13:21:05.702745","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-05-19T03:34:30.701032Z","iopub.execute_input":"2022-05-19T03:34:30.701526Z","iopub.status.idle":"2022-05-19T03:34:30.715627Z","shell.execute_reply.started":"2022-05-19T03:34:30.701484Z","shell.execute_reply":"2022-05-19T03:34:30.714772Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Utils -- Attack\n* Recall\n  * ToTensor() can be seen as a function where $T(x) = x/255$.\n  * Normalize() can be seen as a function where $N(x) = (x-mean)/std$ where $mean$ and $std$ are constants.\n\n* Inverse function\n  * Inverse Normalize() can be seen as a function where $N^{-1}(x) = x*std+mean$ where $mean$ and $std$ are constants.\n  * Inverse ToTensor() can be seen as a function where $T^{-1}(x) = x*255$.\n\n* Special Noted\n  * ToTensor() will also convert the image from shape (height, width, channel) to shape (channel, height, width), so we also need to transpose the shape back to original shape.\n  * Since our dataloader samples a batch of data, what we need here is to transpose **(batch_size, channel, height, width)** back to **(batch_size, height, width, channel)** using np.transpose.","metadata":{"id":"fYCEQwmcrmH6","papermill":{"duration":0.028314,"end_time":"2022-05-17T13:21:05.835829","exception":false,"start_time":"2022-05-17T13:21:05.807515","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# perform adversarial attack and generate adversarial examples\ndef gen_adv_examples(model, loader, attack, loss_fn):\n    model.eval()\n    adv_names = []\n    train_acc, train_loss = 0.0, 0.0\n    for i, (x, y) in enumerate(loader):\n        x, y = x.to(device), y.to(device)\n        x_adv = attack(model, x, y, loss_fn) # obtain adversarial examples\n        yp = model(x_adv)\n        loss = loss_fn(yp, y)\n        train_acc += (yp.argmax(dim=1) == y).sum().item()\n        train_loss += loss.item() * x.shape[0]\n        # store adversarial examples\n        adv_ex = ((x_adv) * std + mean).clamp(0, 1) # to 0-1 scale\n        adv_ex = (adv_ex * 255).clamp(0, 255) # 0-255 scale\n        adv_ex = adv_ex.detach().cpu().data.numpy().round() # round to remove decimal part\n        adv_ex = adv_ex.transpose((0, 2, 3, 1)) # transpose (bs, C, H, W) back to (bs, H, W, C)\n        adv_examples = adv_ex if i == 0 else np.r_[adv_examples, adv_ex]\n    return adv_examples, train_acc / len(loader.dataset), train_loss / len(loader.dataset)\n\n# create directory which stores adversarial examples\ndef create_dir(data_dir, adv_dir, adv_examples, adv_names):\n    if os.path.exists(adv_dir) is not True:\n        _ = shutil.copytree(data_dir, adv_dir)\n    for example, name in zip(adv_examples, adv_names):\n        im = Image.fromarray(example.astype(np.uint8)) # image pixel value should be unsigned int\n        im.save(os.path.join(adv_dir, name))","metadata":{"id":"w5X_9x-7ro_w","papermill":{"duration":0.042451,"end_time":"2022-05-17T13:21:05.906082","exception":false,"start_time":"2022-05-17T13:21:05.863631","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-05-19T03:34:33.843085Z","iopub.execute_input":"2022-05-19T03:34:33.843851Z","iopub.status.idle":"2022-05-19T03:34:33.855615Z","shell.execute_reply.started":"2022-05-19T03:34:33.843809Z","shell.execute_reply":"2022-05-19T03:34:33.854033Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model / Loss Function\n\nModel list is available [here](https://github.com/osmr/imgclsmob/blob/master/pytorch/pytorchcv/model_provider.py). Please select models which has _cifar10 suffix. Some of the models cannot be accessed/loaded. You can safely skip them since TA's model will not use those kinds of models.","metadata":{"id":"r_pMkmPytX3k","papermill":{"duration":0.027118,"end_time":"2022-05-17T13:21:05.961785","exception":false,"start_time":"2022-05-17T13:21:05.934667","status":"completed"},"tags":[]}},{"cell_type":"code","source":"from pytorchcv.model_provider import get_model as ptcv_get_model\n\nmodel = ptcv_get_model('resnet110_cifar10', pretrained=True).to(device)\nloss_fn = nn.CrossEntropyLoss()\n\nbenign_acc, benign_loss = epoch_benign(model, adv_loader, loss_fn)\nprint(f'benign_acc = {benign_acc:.5f}, benign_loss = {benign_loss:.5f}')","metadata":{"id":"jwto8xbPtYzQ","outputId":"f6708aae-e5e4-42f5-d581-7367440d432f","papermill":{"duration":7.92442,"end_time":"2022-05-17T13:21:13.914217","exception":false,"start_time":"2022-05-17T13:21:05.989797","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-05-19T03:34:36.288684Z","iopub.execute_input":"2022-05-19T03:34:36.289436Z","iopub.status.idle":"2022-05-19T03:34:37.173297Z","shell.execute_reply.started":"2022-05-19T03:34:36.289374Z","shell.execute_reply":"2022-05-19T03:34:37.172512Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## FGSM","metadata":{"id":"uslb7GPchtMI","papermill":{"duration":0.029587,"end_time":"2022-05-17T13:21:13.974023","exception":false,"start_time":"2022-05-17T13:21:13.944436","status":"completed"},"tags":[]}},{"cell_type":"code","source":"adv_examples, fgsm_acc, fgsm_loss = gen_adv_examples(model, adv_loader, fgsm, loss_fn)\nprint(f'fgsm_acc = {fgsm_acc:.5f}, fgsm_loss = {fgsm_loss:.5f}')\n\ncreate_dir(root, 'fgsm', adv_examples, adv_names)","metadata":{"id":"wQwPTVUIhuTS","papermill":{"duration":0.035444,"end_time":"2022-05-17T13:21:14.038417","exception":false,"start_time":"2022-05-17T13:21:14.002973","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-05-19T03:33:50.935006Z","iopub.status.idle":"2022-05-19T03:33:50.935314Z","shell.execute_reply.started":"2022-05-19T03:33:50.935156Z","shell.execute_reply":"2022-05-19T03:33:50.935177Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## I-FGSM","metadata":{"id":"WXw6p0A6shZm","papermill":{"duration":0.028988,"end_time":"2022-05-17T13:21:14.096297","exception":false,"start_time":"2022-05-17T13:21:14.067309","status":"completed"},"tags":[]}},{"cell_type":"code","source":"adv_examples, ifgsm_acc, ifgsm_loss = gen_adv_examples(model, adv_loader, ifgsm, loss_fn)\nprint(f'ifgsm_acc = {ifgsm_acc:.5f}, ifgsm_loss = {ifgsm_loss:.5f}')\n\ncreate_dir(root, 'ifgsm', adv_examples, adv_names)","metadata":{"id":"fUEsT06Iskt2","papermill":{"duration":0.035239,"end_time":"2022-05-17T13:21:14.160605","exception":false,"start_time":"2022-05-17T13:21:14.125366","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-05-19T03:33:50.936989Z","iopub.status.idle":"2022-05-19T03:33:50.937708Z","shell.execute_reply.started":"2022-05-19T03:33:50.937408Z","shell.execute_reply":"2022-05-19T03:33:50.937481Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Compress the images\n* Submit the .tgz file to [JudgeBoi](https://ml.ee.ntu.edu.tw/hw10/)","metadata":{"id":"DQ-nYkkYexEE","papermill":{"duration":0.029188,"end_time":"2022-05-17T13:21:14.218887","exception":false,"start_time":"2022-05-17T13:21:14.189699","status":"completed"},"tags":[]}},{"cell_type":"code","source":"%cd fgsm\n!tar zcvf ../fgsm.tgz *\n%cd ..\n\n%cd ifgsm\n!tar zcvf ../ifgsm.tgz *\n%cd ..","metadata":{"id":"ItRo_S0M264N","papermill":{"duration":0.035403,"end_time":"2022-05-17T13:21:14.283535","exception":false,"start_time":"2022-05-17T13:21:14.248132","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-05-19T03:33:50.939115Z","iopub.status.idle":"2022-05-19T03:33:50.939832Z","shell.execute_reply.started":"2022-05-19T03:33:50.93956Z","shell.execute_reply":"2022-05-19T03:33:50.939589Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Ensemble Attack\n* Ensemble multiple models as your proxy model to increase the black-box transferability ([paper](https://arxiv.org/abs/1611.02770))","metadata":{"id":"WLZLbebigCA2","papermill":{"duration":0.029201,"end_time":"2022-05-17T13:21:14.342263","exception":false,"start_time":"2022-05-17T13:21:14.313062","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# model_names = ['resnet20_cifar10',\n#                'resnet56_cifar10',\n#                'resnet164bn_cifar10',\n#                'preresnet20_cifar10',\n#                'preresnet56_cifar10',\n#                'preresnet164bn_cifar10',\n#                'densenet40_k12_cifar10',\n#               'densenet100_k12_cifar10',\n#                'densenet250_k24_bc_cifar10',\n#               'diapreresnet56_cifar10',\n#                'diapreresnet110_cifar10',\n#                'sepreresnet56_cifar10',\n#                'sepreresnet110_cifar10',\n#                'diaresnet56_cifar10'\n#               ] 0.22\n\n# model_names = ['resnext29_16x64d_cifar10',\n#                 'resnet56_cifar10',\n#                'resnet20_cifar10',\n#                'preresnet56_cifar10',\n#                'preresnet110_cifar10',\n#                'preresnet20_cifar10',\n#                'seresnet110_cifar10',\n#                'sepreresnet56_cifar10',\n#                'sepreresnet110_cifar10',\n#                'diaresnet56_cifar10',\n#                'xdensenet40_2_k24_bc_cifar10',\n#                'diapreresnet56_cifar10',\n#                'diapreresnet110_cifar10',\n#               ]0.18\nmodel_names = ['resnext29_16x64d_cifar10',\n                'resnet56_cifar10',\n               'resnet20_cifar10',\n               'preresnet56_cifar10',\n               'preresnet110_cifar10',\n               'preresnet20_cifar10',\n               'seresnet110_cifar10',\n               'seresnet56_cifar10',\n               'sepreresnet56_cifar10',\n               'sepreresnet110_cifar10',\n               'diaresnet56_cifar10',\n               'diaresnet110_cifar10',\n               'xdensenet40_2_k24_bc_cifar10',\n               'diapreresnet56_cifar10',\n               'diapreresnet110_cifar10',\n              ]0.20\nnum= len(model_names)","metadata":{"id":"stYFytogeIzI","papermill":{"duration":0.036203,"end_time":"2022-05-17T13:21:14.408356","exception":false,"start_time":"2022-05-17T13:21:14.372153","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-05-19T03:34:42.885151Z","iopub.execute_input":"2022-05-19T03:34:42.885538Z","iopub.status.idle":"2022-05-19T03:34:42.889702Z","shell.execute_reply.started":"2022-05-19T03:34:42.885497Z","shell.execute_reply":"2022-05-19T03:34:42.888883Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Construct your ensemble model","metadata":{"id":"yjfJwJKeeaR2","papermill":{"duration":0.030527,"end_time":"2022-05-17T13:21:14.468733","exception":false,"start_time":"2022-05-17T13:21:14.438206","status":"completed"},"tags":[]}},{"cell_type":"code","source":"class ensembleNet(nn.Module):\n    def __init__(self, model_names):\n        super().__init__()\n        self.models = nn.ModuleList([ptcv_get_model(name, pretrained=True) for name in model_names])\n        \n    def forward(self, x):\n        ensemble_logits = 0\n        for i, m in enumerate(self.models):\n        # TODO: sum up logits from multiple models\n          ensemble_logits = ensemble_logits +self.models[i](x) / num\n        return ensemble_logits","metadata":{"id":"gJcKiQNUgnPQ","papermill":{"duration":0.039718,"end_time":"2022-05-17T13:21:14.539667","exception":false,"start_time":"2022-05-17T13:21:14.499949","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-05-19T03:34:45.919784Z","iopub.execute_input":"2022-05-19T03:34:45.920134Z","iopub.status.idle":"2022-05-19T03:34:45.926293Z","shell.execute_reply.started":"2022-05-19T03:34:45.920083Z","shell.execute_reply":"2022-05-19T03:34:45.925342Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nensemble_model = ensembleNet(model_names).to(device)\nensemble_model.eval()\nloss_fn = nn.CrossEntropyLoss()\n\nadv_examples, ensemble_acc, ensemble_loss = gen_adv_examples(ensemble_model, adv_loader, mifgsm, loss_fn)\nprint(f'ensemble_acc = {ensemble_acc:.5f}, ensemble_loss = {ensemble_loss:.5f}')\n\ncreate_dir(root, 'ensemble', adv_examples, adv_names)","metadata":{"id":"sb1bFx75IUji","outputId":"1c016b51-2ebe-454e-e61b-cc3038ad72df","papermill":{"duration":896.960085,"end_time":"2022-05-17T13:36:11.530347","exception":false,"start_time":"2022-05-17T13:21:14.570262","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-05-19T03:34:49.076878Z","iopub.execute_input":"2022-05-19T03:34:49.077437Z","iopub.status.idle":"2022-05-19T03:42:08.344581Z","shell.execute_reply.started":"2022-05-19T03:34:49.0774Z","shell.execute_reply":"2022-05-19T03:42:08.343789Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%cd ensemble\n!tar zcvf ../ensemble.tgz *\n%cd ..","metadata":{"id":"3CiK5OiDEYh0","papermill":{"duration":0.833588,"end_time":"2022-05-17T13:36:12.396729","exception":false,"start_time":"2022-05-17T13:36:11.563141","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-05-19T04:00:13.04357Z","iopub.execute_input":"2022-05-19T04:00:13.043875Z","iopub.status.idle":"2022-05-19T04:00:14.390712Z","shell.execute_reply.started":"2022-05-19T04:00:13.043834Z","shell.execute_reply":"2022-05-19T04:00:14.389853Z"},"trusted":true},"execution_count":null,"outputs":[]}]}